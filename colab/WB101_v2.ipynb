{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/timsetsfire/wandb-examples/blob/main/colab/WB101_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Weights & Biases 101 ü•æüèïÔ∏è\n",
        "\n",
        "This notebook is intended to show you how to track your machine learning experiments using [Weights & Biases](https://wandb.ai).\n",
        "\n",
        "Weights & Biases has two major components: a python client named `wandb` ü™Ñüêù  and a web application that allows you to store, query, visualize, and share metadata from your machine learning experiments, e.g. loss curves, evaluation metrics, model predictions... you can `wandb.log` *just about* anything.\n",
        "\n",
        "The client is open source and you can find the [source code on Github](http://github.com/wandb/wandb)! ‚≠ê\n",
        "\n",
        "The web application is publicly hosted at [wandb.ai](http://wandb.ai), but the app can be deployed in private environments as well. If you're interested in learning more about private deployments of the web app, check out [these docs](http://docs.wandb.ai/guides/self-hosted). \n",
        "\n",
        "The first step on our journey is to install the client, which is as easy as:"
      ],
      "metadata": {
        "id": "SMSSb5vCs2q0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install wandb keplergl\n",
        "!pip install --upgrade xgboost"
      ],
      "metadata": {
        "id": "DOJPiyNAu4lv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Client Configuration\n",
        "\n",
        "The first thing we do is set the `WANDB_PROJECT` environment variable. This tells the client to send all the data we log to a specific workspace in Weights & Biases. Environment variables are a great way to customize the behavior of the client without having to hardcode contextual details like a project name. You can check out [these docs](https://docs.wandb.ai/guides/track/advanced/environment-variables) for a complete list of the environment variables you can use to configure `wandb`."
      ],
      "metadata": {
        "id": "kIwEgFsVnhbr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"WANDB_PROJECT\"] = \"bootcamp\"\n",
        "# os.environ[\"WANDB_ENTITY\"] = \"your-entity\"\n",
        "# os.environ[\"WANDB_BASE_URL\"] = \"your-url\""
      ],
      "metadata": {
        "id": "9HNLLjQV4rzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we need to connect the client to an account in the web server. We do this in a notebook by calling `wandb.login`.\n",
        "\n",
        "If you have Google SSO enabled for your Weights & Biases account and are running this notebook on colab, the authentication happens automatically when `wandb.login` is called.\n",
        "\n",
        "Otherwise, you will see a authoriztion link and be asked to enter an API key. If you already have an account, you can follow the authorization link and then copy and paste the displayed API key. If you don't have an account you can sign up with an email address or using a Google or Github account."
      ],
      "metadata": {
        "id": "BYTm6GoRINMx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "\n",
        "# Equivalent to running \"wandb login\" in your shell\n",
        "# \n",
        "wandb.login()\n",
        "# \n",
        "# Note that https://api.wandb.ai is the default and points to the publicly hosted\n",
        "# app. You'll want to change this to a different API endpoint if you are trying\n",
        "# to connect to a privately hosted server.\n",
        "# \n",
        "# Alternative you can configure this with environment variables:\n",
        "# export WANDB_API_KEY=\"<your-api-key>\"\n",
        "# export WANDB_BASE_URL=\"<your-wandb-endpoint>\""
      ],
      "metadata": {
        "id": "JsNzSCjg7rD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calling `wandb login` or `wandb.login` will write your API key to your `~/.netrc` file. __To authenticate the client in a headless job on the cloud, you will definitely want to use the `WANDB_API_KEY` environment variable__."
      ],
      "metadata": {
        "id": "zer98IpzJ-Df"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Track your Experiments\n",
        "\n",
        "### `wandb.init`\n",
        "\n",
        "The `wandb.init` function initializes a new `Run`, which you can think of as a comprehensive record of your machine learning experiment. Tracking starts when you call `wandb.init` and ends when you call `wandb.finish` (called automatically via `atexit` hooks if you don't want to invoke manually). You can also use python's `with` statement to initialize and finish runs (see code cell below).\n",
        "\n",
        "### `wandb.log`\n",
        "\n",
        "You can call `wandb.log` within your experiment add metrics to your `Run`. The idea is that you will call `wandb.log` many times over an experiment for the same metric, in which case the run saves the whole history of each metric across all of your `wandb.log` calls. The code cell below demonstrates how this looks in a typical stochastic gradient descent loop.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kCvJbBYOnnUv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%wandb\n",
        "\n",
        "config = dict(\n",
        "  batch_size=32,\n",
        "  learning_rate=1e-4,\n",
        "  flux_capacitors=64,\n",
        "  life_universe_everything=42,\n",
        ")\n",
        "\n",
        "\"\"\"\n",
        "The pattern of \"with wandb.init()...\" causes wandb.finish() to be called as\n",
        "soon as we leave the with block. This is especially useful when you have a script\n",
        "or notebook that initializes multiple runs that you want to track separately.\n",
        "\"\"\"\n",
        "with wandb.init(config=config, job_type = \"general-logging\"):\n",
        "\n",
        "    for key, value in dict(wandb.config).items():\n",
        "        print(key, value)\n",
        "\n",
        "    # Imagine we run 100 epochs of model training\n",
        "    for x in range(2, 100):\n",
        "\n",
        "        # Insert model training here...\n",
        "        # ...\n",
        "\n",
        "        # Compute metrics (or in this case, make them up)\n",
        "        metrics = dict(\n",
        "            loss=(1/x)**0.25,\n",
        "            accuracy=1-(1/x)*2\n",
        "        )\n",
        "\n",
        "        # Pass metrics to Weights & Biases\n",
        "        wandb.log(metrics)\n"
      ],
      "metadata": {
        "id": "EtgR9ypudbno"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Anatomy of a `Run` ü©∫\n",
        "\n",
        "The `Run` stores a detailed record of an experiment within a few specific data structures. The important things to know about are\n",
        "- `Run.config` is a dictionary like structure that stores configuration data for a run, like the path to input data or training hyperparameters. You can instatiate the config by passing a dictionary to `wandb.init(config=<config-dict>)`.\n",
        "- `Run.history` is a list of dictionaries that stores historical values of metrics and media over the course of an experiment. We can append a new snapshot of our training metrics by calling `wandb.log(<metric-dict>)`\n",
        "- `Run.summary` is a dictionary for recording summary metrics or media. By default the `summary` will contain the most recent values logged for each metric, you can overwrite and add elements as you like.\n"
      ],
      "metadata": {
        "id": "8o1xoifVp2Av"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Log Visualizations\n",
        "\n",
        "The example we just ran shows you can log numerical metadata over the course of an experiment to Weights & Biases, and we saw how metrics like loss and accuracy show up as customizable line plots in the app. Now let's take a look at some of the other things you can `wandb.log` into your record of an experiment. You can see examples below, but for a first list of loggable types check out [these docs](https://docs.wandb.ai/ref/python/data-types)."
      ],
      "metadata": {
        "id": "imQtmxy_nuzK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Log Plotly\n",
        "\n",
        "Plotly is a popular library for building and rendering custom visualizations. It is tightly connected to the ubiquitous `matplotlib` package. We integrate with Plotly so that you can pass `matplotlib` or Plotly figures directly to `wandb.log`, which will automatically serialize and upload your figures to Weights & Biases, where they will be interactively rendered inside of your experiment records. You can run the cells below to see an example logging an interactive, 3D visualization to W&B."
      ],
      "metadata": {
        "id": "iw-e6V57BvIG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px"
      ],
      "metadata": {
        "id": "0hSxatyx2wH2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%wandb\n",
        "\n",
        "with wandb.init(job_type = \"plotly-logging\") as run:\n",
        "  \n",
        "  # Loads Iris dataset\n",
        "  df = px.data.iris()\n",
        "\n",
        "  # Construct a 3D scatter plot with plotly\n",
        "  fig = px.scatter_3d(df, \n",
        "                      x='sepal_length', \n",
        "                      y='sepal_width', \n",
        "                      z='petal_width',\n",
        "                      color='species')\n",
        "  \n",
        "  # Log figure\n",
        "  run.log({\"figure\": fig})"
      ],
      "metadata": {
        "id": "6QlDM4C-PBz1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Log Dataframes\n",
        "\n",
        "You can also log `pandas.DataFrame` objects with `wandb.log`! These will be converted into a `wandb.Table` (docs) and interactievly displayed inside of W&B. The cell below logs the entire Iris dataset and renders it in the W&B app üòé\n",
        "\n",
        "Note: One of the most powerful features of `wandb.Table`s is that you can include any `wandb` type as a cell value! This includes, images, plots, videos, audio... almost anything ü§©"
      ],
      "metadata": {
        "id": "riP-pAODp9b3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%wandb\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "data = fetch_california_housing(as_frame=True)\n",
        "df = data.data\n",
        "df[data.target.name] = data.target\n",
        "\n",
        "with wandb.init(job_type = \"table-logging\") as run:\n",
        "  # Log a pandas Dataframe to Weights & Biases\n",
        "  wandb.log({\"california\": df})"
      ],
      "metadata": {
        "id": "oEjawsA9qCJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Log HTML\n",
        "\n",
        "You can use the `wandb.Html` class to pass in any HTML and have it saved and rendered in Weights & Biases. This allows indirect integration with all sorts of visualization libraries, like `keplergl`."
      ],
      "metadata": {
        "id": "D5Y8D63GoHtA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%wandb\n",
        "import keplergl\n",
        "with wandb.init(job_type = \"html-logging\") as run:\n",
        "  m = keplergl.KeplerGl(height = 400, data = {\"data\": df})\n",
        "  m.save_to_html(file_name = \"point_map.html\")\n",
        "  run.log({\"california housing prices map\": wandb.Html(open(\"point_map.html\"))})"
      ],
      "metadata": {
        "id": "jLVaa8xt2nHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%wandb -h 800\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "with wandb.init(job_type = \"html-logging\") as run:\n",
        "\n",
        "  # Process text and produce html visualizations\n",
        "  text = \"You can use Weights & Biases to log html, it's pretty wild.\"\n",
        "  nlp = spacy.load(\"en_core_web_sm\")\n",
        "  doc = nlp(text)\n",
        "  dep = displacy.render(doc, style=\"dep\")\n",
        "  # Log html to Weights & Biases\n",
        "  run.log({\"DEP\": wandb.Html(dep)})"
      ],
      "metadata": {
        "id": "tAvoFgQHlNaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Log Sequences of Media\n",
        "\n",
        "If you periodically `wandb.log` a number (for example, loss), Weights & Biases will automatically render a line plot showing the change in that value over time (a loss curve). You can also log media under a key more than once over the course of an experiment, in which case Weights & Biases will display that media with a step slider so you can scrub over the course of the experiment and see how it changed. This is particularly useful for seeing how model predictions and visualizations of model performance (e.g. a precision/recall curve) change over time. In the example below, we log a `wandb.Image` repeatedly after applying a blur operation between each log step, just to demonstrate how this works."
      ],
      "metadata": {
        "id": "TwCPIt7Yn225"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image, ImageFilter"
      ],
      "metadata": {
        "id": "nCHqt3tQ280I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%wandb\n",
        "\n",
        "# Downloads image\n",
        "!curl https://parade.com/.image/t_share/MTkwNTgwOTUyNjU2Mzg5MjQ1/albert-einstein-quotes-jpg.jpg > image.jpg\n",
        "\n",
        "# Load image with pillow, resize to 512 square\n",
        "im = Image.open(\"./image.jpg\").resize((512, 512))\n",
        "\n",
        "with wandb.init(job_type = \"image-logging\") as run:\n",
        "\n",
        "  for _ in range(15):\n",
        "    \n",
        "    # Log image\n",
        "    run.log({\"image\": wandb.Image(im)})\n",
        "    \n",
        "    # Apply small Gaussian blur\n",
        "    im = im.filter(ImageFilter.GaussianBlur(radius=1.5))\n",
        "\n",
        "    \n"
      ],
      "metadata": {
        "id": "WaIyQCp6G4Bc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## XGBoost\n",
        "\n",
        "Some machine learning frameworks don't ask the user to write the training loop themselves, but instead allow customization of a standard training loop through hooks and callbacks, for example fastai, pytorch lightning, keras, and xgboost. Such frameworks also provide basic mechanisms for defining and reporting metrics throughout training.\n",
        "\n",
        "For all the frameworks above (and many more) Weights & Biases provides an out of the box integration. The cells below use XGBoost to train a regressor that predicts the price of a home in CA from a standard dataset. By providing the `wandb.xgboost.WandbCallback` when we train the regressor, all evaluation metrics defined through the standard XGBoost interface and feature importances will be recorded to W&B automatically."
      ],
      "metadata": {
        "id": "ZhvxW3Y0jh4b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "import wandb\n",
        "from wandb.xgboost import WandbCallback\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "\n",
        "with wandb.init(job_type = \"xgb-training\"):\n",
        "  # Fetch the CA housing dataset\n",
        "  data = fetch_california_housing(as_frame=True)\n",
        "  df = data.data\n",
        "  df[data.target.name] = data.target\n",
        "\n",
        "  df_train, df_valid = train_test_split(df, test_size=0.2)\n",
        "\n",
        "  features = [\"HouseAge\",\t\"AveRooms\",\t\"AveBedrms\",\t\"Population\",\t\"AveOccup\",\t\"Latitude\",\t\"Longitude\"]\n",
        "  label = \"MedHouseVal\"\n",
        "  # Construct DMatrix objects from our dataframe\n",
        "  train_data = xgb.DMatrix(df_train[features], label=df_train[label])\n",
        "  valid_data = xgb.DMatrix(df_valid[features], label=df_valid[label])\n",
        "\n",
        "  xgb.train(\n",
        "      {}, \n",
        "      train_data, \n",
        "      20, \n",
        "      evals=[(valid_data, \"validation\")], \n",
        "      callbacks=[WandbCallback(log_model=True)]\n",
        "  )"
      ],
      "metadata": {
        "id": "oh3z7GWX_9xu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pytorch\n",
        "\n",
        "Now let's take a look at some code that actually trains a neural network and logs metrics and predictions to Weights & Biases. The code below trains a simple convolutional neural network (CNN) to classify images of clothing from the Fashion MNIST dataset. There are a few cool ways that the code below uses `wandb` to keep track of each execution.\n",
        "\n",
        "1. `wandb.watch` - This function from the `wandb` library allows you to automatically record gradients, parameter distributions, and network topology for any `torch` based model.\n",
        "\n",
        "2. Every 25 training steps, we log the average loss over the previous 25 steps as well as a `wandb.Histogram` the losses recorded in each of the previous 25 batches, which will allow us to see how the distribution of batch losses shifts over the course of training.\n",
        "\n",
        "3. We also load the examples and predictions from the first batch of every epoch into a `wandb.Table`.\n",
        "\n"
      ],
      "metadata": {
        "id": "dwbat9WvoXDl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import FashionMNIST"
      ],
      "metadata": {
        "id": "MVIva1fL3K3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FashionCNN(nn.Module):\n",
        "  \"\"\"Simple CNN for Fashion MNIST.\"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.layer1 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1),\n",
        "        nn.BatchNorm2d(32),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "    )\n",
        "    self.layer2 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3),\n",
        "        nn.BatchNorm2d(64),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(2)\n",
        "    )    \n",
        "    self.fc1 = nn.Linear(in_features=64*6*6, out_features=600)\n",
        "    self.drop = nn.Dropout2d(0.25)\n",
        "    self.fc2 = nn.Linear(in_features=600, out_features=120)\n",
        "    self.fc3 = nn.Linear(in_features=120, out_features=10) \n",
        "  \n",
        "  def forward(self, x):\n",
        "    out = self.layer1(x)\n",
        "    out = self.layer2(out)\n",
        "    out = out.view(out.size(0), -1)\n",
        "    out = self.fc1(out)\n",
        "    out = self.drop(out)\n",
        "    out = self.fc2(out)\n",
        "    out = self.fc3(out)\n",
        "    return out\n",
        "\n",
        "\n",
        "def train_fmnist(config):\n",
        "  # Pass config into wandb.init\n",
        "  with wandb.init(config=config, job_type = \"fmnist-training\") as run:\n",
        "    \n",
        "    # Training setup\n",
        "    config = run.config\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = FashionCNN()\n",
        "    model.to(device)\n",
        "    train_dataset = FashionMNIST(\"./data/\", download=True, train=True, transform=transforms.ToTensor())\n",
        "    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, pin_memory=True)\n",
        "    error = nn.CrossEntropyLoss()\n",
        "    learning_rate = config.learning_rate\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # We can pass our network to wandb.watch and automatically log gradients, \n",
        "    # weights, topology, and more...\n",
        "    run.watch(model, log=\"all\", log_graph=True)\n",
        "\n",
        "    # Epoch loop\n",
        "    iter = 0\n",
        "    losses = []\n",
        "    for epoch in range(config.epochs):\n",
        "\n",
        "      # Iterate over batches of the data\n",
        "      for idx, (images, labels) in enumerate(train_loader):\n",
        "\n",
        "        iter += 1\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        loss = error(outputs, labels)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        if iter % 25 == 1:\n",
        "          run.log(\n",
        "            {\n",
        "              \"train/loss\": sum(losses)/len(losses),  # Log average loss\n",
        "              \"train/losses\": wandb.Histogram(losses)  # Log all losses\n",
        "            }\n",
        "          )\n",
        "          losses = []\n",
        "\n",
        "        # Log the predictions from the first training batch as a wandb.Table\n",
        "        if idx == 0:\n",
        "          table = wandb.Table(columns=[\"image\", \"label\", \"prediction\"])\n",
        "          for im, lab, pred in zip(images, labels, outputs):\n",
        "            pred = torch.argmax(pred)\n",
        "            table.add_data(wandb.Image(im.cpu()), lab.item(), pred.item())\n",
        "          run.log({\"train/predictions\": table})\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ],
      "metadata": {
        "id": "6UnntHu-nnwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training hyperparamters\n",
        "config = {\n",
        "  \"learning_rate\": 0.0001,\n",
        "  \"batch_size\": 256,\n",
        "  \"epochs\": 5,\n",
        "}\n",
        "train_fmnist(config)"
      ],
      "metadata": {
        "id": "odx_dXBPDC5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tensorflow\n",
        "\n",
        "You can, of course, add `wandb.init` and `wandb.log` calls to track any python compute, including a job that trains a model using `tensorflow`. However, if you already have `tensorboard` logging set up in a `tensorflow` project, you can simply pass the `sync_tensorboard=True` argument to `wandb.init` and all metrics written to tensorboard will be automatically synced to Weights & Biases!"
      ],
      "metadata": {
        "id": "n4tXDI8uqfOc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "zsUTAV4t3cwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(config):\n",
        "  \"\"\"Construct a simple neural network.\"\"\"\n",
        "  model = tf.keras.models.Sequential(\n",
        "    [\n",
        "      tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "      tf.keras.layers.Dense(config.layer_1, activation=config.activation_1),\n",
        "      tf.keras.layers.Dropout(config.dropout),\n",
        "      tf.keras.layers.Dense(config.layer_2, activation=config.activation_2),\n",
        "    ]\n",
        "  )\n",
        "  model.compile(optimizer=config.optimizer, loss=config.loss, metrics=[config.metric])\n",
        "  return model\n",
        "\n",
        "\n",
        "def train_mnist():\n",
        "\n",
        "  # Default training hyperparameters\n",
        "  config = {\n",
        "    \"layer_1\": 512,\n",
        "    \"activation_1\": \"relu\",\n",
        "    \"dropout\": 0.25,\n",
        "    \"layer_2\": 10,\n",
        "    \"activation_2\": \"softmax\",\n",
        "    \"optimizer\": \"sgd\",\n",
        "    \"loss\": \"sparse_categorical_crossentropy\",\n",
        "    \"metric\": \"accuracy\",\n",
        "    \"epoch\": 5,\n",
        "    \"batch_size\": 256,\n",
        "  }\n",
        "\n",
        "  # Passes in config and sets sync_tensorboard=True\n",
        "  with wandb.init(config=config, sync_tensorboard=True) as run:\n",
        "      \n",
        "      # Build model and prepare data\n",
        "      model = build_model(run.config)\n",
        "      mnist = tf.keras.datasets.mnist\n",
        "      (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "      \n",
        "      # Trains model and passes in TensorBoard callback. This will log metrics\n",
        "      # and weight histograms. Because we set sync_tensorboard=True, the tensorboard\n",
        "      # files will be passed to Weights & Biases.\n",
        "      model.fit(\n",
        "          x=x_train,\n",
        "          y=y_train,\n",
        "          epochs=run.config.epoch,\n",
        "          batch_size=run.config.batch_size,\n",
        "          validation_data=(x_test[:100], y_test[:100]),\n",
        "          callbacks=[tf.keras.callbacks.TensorBoard(histogram_freq=1)],\n",
        "      )\n",
        "\n",
        "      # Log model weights to Weights & Biases as an Artifact\n",
        "      model.save(\"model.keras\")\n",
        "      model_artifact = wandb.Artifact(name=\"mnist-model\", type=\"model\")\n",
        "      model_artifact.add_file(\"model.keras\")\n",
        "      # If you are writing model to cloud storage, you can track it with:\n",
        "      # model_artifact.add_reference(\"s3://<your-bucket>/.../model.keras\")\n",
        "      # or\n",
        "      # model_artifact.add_reference(\"gs://<your-bucket>/.../model.keras\")\n",
        "      run.log_artifact(model_artifact)\n"
      ],
      "metadata": {
        "id": "pmNEFnLV9oQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_mnist()"
      ],
      "metadata": {
        "id": "67GYLOteER7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running a Sweep üßπ\n",
        "\n",
        "Now that your hyperparameters are stored in the `wandb.config`, you can use Sweeps to automatically tune your hyperparameters. There are basically two steps\n",
        "1. Create and upload a sweep config.\n",
        "2. Start a sweep agent or agents.\n",
        "\n",
        "You can do (1) from a notebook by passing a dictionary config to `wandb.sweep` or on from the command line with a yaml config by running `wandb sweep config.yaml`.\n",
        "\n",
        "You can do (2) from a notebook by your train function to `wandb.agent` or from the command line by specifying a program name in your config that will be run by invoking `wandb agent`.\n"
      ],
      "metadata": {
        "id": "0Cxzk9hS_VOt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_config = {\n",
        "  'method': 'bayes',\n",
        "  'metric': {\n",
        "    'name': 'validation/epoch_loss',\n",
        "    'goal': 'minimize'   \n",
        "  },\n",
        "  'parameters': {\n",
        "    'dropout': {\n",
        "      'distribution': 'uniform',\n",
        "      'min': 0.25,\n",
        "      'max': 0.5\n",
        "    },\n",
        "    'batch_size': {\n",
        "      'values': [128, 256]\n",
        "    },\n",
        "    'epochs':{\n",
        "      'distribution': 'int_uniform',\n",
        "      'max': 6,\n",
        "      'min': 3\n",
        "    },\n",
        "    'optimizer': {\n",
        "        'values': ['adam', 'sgd']\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "# Construct a sweep\n",
        "sweep_id = wandb.sweep(sweep_config)"
      ],
      "metadata": {
        "id": "VsOQSMMtAvSV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.agent(sweep_id, train_mnist, count = 2)"
      ],
      "metadata": {
        "id": "gOwidR44CPJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mVcXrsrw2ePy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tJRLBl7c2cey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K_lsCD2pksSy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Jz2OFLGx88KA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}