{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/timsetsfire/wandb-examples/blob/main/colab/Sweeps_with_Pytorch_Lightning_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bKUw6leBWslb"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install -q pytorch-lightning wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sj10iJjUWslb"
      },
      "source": [
        "We make sure we're logged into W&B so that our experiments can be associated with our account."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2BzHVzxWslb",
        "outputId": "3f14e9eb-17da-4102-a62d-6c2f725579f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtim-w\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import wandb\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "TA904c-XWsle"
      },
      "outputs": [],
      "source": [
        "accuracy?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import SGD\n",
        "from torch.optim import Adagrad\n",
        "import torch\n",
        "from torch.nn import Linear, CrossEntropyLoss, functional as F\n",
        "from torch.optim import Adam\n",
        "from torchmetrics.functional import accuracy\n",
        "from pytorch_lightning import LightningModule\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import os\n",
        "import glob\n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "from pytorch_lightning import Trainer\n",
        "from pytorch_lightning.callbacks import Callback\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "\n",
        "transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.1307,), (0.3081,))])\n",
        "\n",
        "dataset = MNIST(root=\"./MNIST\", download=True, transform=transform)\n",
        "training_set, validation_set = random_split(dataset, [55000, 5000])\n",
        "\n",
        "class MNIST_LitModule(LightningModule):\n",
        "\n",
        "    def __init__(self, n_classes=10, n_layer_1=128, n_layer_2=256, lr=1e-3, optim = \"Adam\"):\n",
        "        '''method used to define our model parameters'''\n",
        "        super().__init__()\n",
        "\n",
        "        # mnist images are (1, 28, 28) (channels, width, height)\n",
        "        self.layer_1 = Linear(28 * 28, n_layer_1)\n",
        "        self.layer_2 = Linear(n_layer_1, n_layer_2)\n",
        "        self.layer_3 = Linear(n_layer_2, n_classes)\n",
        "\n",
        "        # loss\n",
        "        self.loss = CrossEntropyLoss()\n",
        "\n",
        "        # optimizer parameters\n",
        "        self.lr = lr\n",
        "\n",
        "        # save hyper-parameters to self.hparams (auto-logged by W&B)\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "        self.optim = optim\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''method used for inference input -> output'''\n",
        "\n",
        "        batch_size, channels, width, height = x.size()\n",
        "\n",
        "        # (b, 1, 28, 28) -> (b, 1*28*28)\n",
        "        x = x.view(batch_size, -1)\n",
        "\n",
        "        # let's do 3 x (linear + relu)\n",
        "        x = self.layer_1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.layer_2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.layer_3(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        '''needs to return a loss from a single batch'''\n",
        "        _, loss, acc = self._get_preds_loss_accuracy(batch)\n",
        "\n",
        "        # Log loss and metric\n",
        "        self.log('train_loss', loss)\n",
        "        self.log('train_accuracy', acc)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        '''used for logging metrics'''\n",
        "        preds, loss, acc = self._get_preds_loss_accuracy(batch)\n",
        "\n",
        "        # Log loss and metric\n",
        "        self.log('val_loss', loss)\n",
        "        self.log('val_accuracy', acc)\n",
        "\n",
        "        # Let's return preds to use it in a custom callback\n",
        "        return preds\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        '''used for logging metrics'''\n",
        "        _, loss, acc = self._get_preds_loss_accuracy(batch)\n",
        "\n",
        "        # Log loss and metric\n",
        "        self.log('test_loss', loss)\n",
        "        self.log('test_accuracy', acc)\n",
        "    \n",
        "    def configure_optimizers(self):\n",
        "        '''defines model optimizer'''\n",
        "        if self.optim == \"Adam\":\n",
        "          return Adam(self.parameters(), lr=self.lr)\n",
        "        elif self.optim == \"SGD\":\n",
        "          return SGD(self.parameters(), lr = self.lr)\n",
        "        elif self.optim == \"Adagrad\":\n",
        "          return Adagrad(self.parameters(), lr = self.lr)\n",
        "    \n",
        "    def _get_preds_loss_accuracy(self, batch):\n",
        "        '''convenience function since train/valid/test steps are similar'''\n",
        "        x, y = batch\n",
        "        logits = self(x)\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        loss = self.loss(logits, y)\n",
        "        acc = accuracy(preds=preds, target=y, task=\"multiclass\", num_classes=10)\n",
        "        return preds, loss, acc"
      ],
      "metadata": {
        "id": "NS1vkFTvmSC5"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rkuBT_okbjtt"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z7wCnCS-bovO"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Fsoy2ppTbolj"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhS-kO3KWslf"
      },
      "source": [
        "## ‚öôÔ∏è Using WandbLogger to log Images, Text and More\n",
        "Pytorch Lightning is extensible through its callback system. We can create a custom callback to automatically log sample predictions during validation. `WandbLogger` provides convenient media logging functions:\n",
        "* `WandbLogger.log_text` for text data\n",
        "* `WandbLogger.log_image` for images\n",
        "* `WandbLogger.log_table` for [W&B Tables](https://docs.wandb.ai/guides/data-vis).\n",
        "\n",
        "An alternate to `self.log` in the Model class is directly using `wandb.log({dict})` or `trainer.logger.experiment.log({dict})`\n",
        "\n",
        "In this case we log the first 20 images in the first batch of the validation dataset along with the predicted and ground truth labels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hr9o_tPAWslf"
      },
      "source": [
        "## üèãÔ∏è‚Äç Set up sweep"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pytorch_lightning.loggers import WandbLogger\n",
        "from pytorch_lightning import Trainer\n",
        "\n",
        "sweep_config = {\n",
        "  'method': 'grid', \n",
        "  'metric': {\n",
        "      'name': 'val_loss',  ## matches what i write via SummaryWriter\n",
        "      'goal': 'minimize'\n",
        "  },\n",
        "  'early_terminate':{\n",
        "      'type': 'hyperband',\n",
        "      'min_iter': 5\n",
        "  },\n",
        "  'parameters': {\n",
        "      'learning_rate':{\n",
        "          'values': [0.05,0.025,0.01,0.005,0.001]\n",
        "      }, \n",
        "      'batch_size': { \n",
        "          'values': [128, 256]\n",
        "      }\n",
        "  }\n",
        "}\n",
        "\n",
        "\n",
        "def sweep_train(config_defaults = dict(learning_rate=0.01, batch_size = 128)): \n",
        "\n",
        "  config_standard = {\n",
        "          \"num_workers\": os.cpu_count(),  # try 0, 1, and 2\n",
        "          \"pin_memory\": True,  # try False and True\n",
        "          \"precision\": 32,  # try 16 and 32\n",
        "          \"optimizer\": \"Adam\", \n",
        "          }\n",
        "  \n",
        "  config = {**config_defaults, **config_standard}\n",
        "  wandb_logger = WandbLogger(log_model='all', save_code=True, config=config)\n",
        "  run = wandb.run \n",
        "  run.mark_preempting()\n",
        "\n",
        "  class LogPredictionsCallback(Callback):\n",
        "      \n",
        "      def on_validation_batch_end(\n",
        "          self, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx):\n",
        "          \"\"\"Called when the validation batch ends.\"\"\"\n",
        "  \n",
        "          # `outputs` comes from `LightningModule.validation_step`\n",
        "          # which corresponds to our model predictions in this case\n",
        "          \n",
        "          # Let's log 20 sample image predictions from first batch\n",
        "          if batch_idx == 0:\n",
        "              n = 20\n",
        "              x, y = batch\n",
        "              images = [img for img in x[:n]]\n",
        "              captions = [f'Ground Truth: {y_i} - Prediction: {y_pred}' for y_i, y_pred in zip(y[:n], outputs[:n])]\n",
        "              \n",
        "              # Option 1: log images with `WandbLogger.log_image`\n",
        "              wandb_logger.log_image(key='sample_images', images=images, caption=captions)\n",
        "\n",
        "              # Option 2: log predictions as a Table\n",
        "              columns = ['image', 'ground truth', 'prediction']\n",
        "              data = [[wandb.Image(x_i), y_i, y_pred] for x_i, y_i, y_pred in list(zip(x[:n], y[:n], outputs[:n]))]\n",
        "              wandb_logger.log_table(key='sample_table', columns=columns, data=data)  \n",
        "\n",
        "  log_predictions_callback = LogPredictionsCallback()\n",
        "  checkpoint_callback = ModelCheckpoint(monitor='val_accuracy', mode='max')\n",
        "\n",
        "  training_loader = DataLoader(training_set, batch_size=wandb.config.batch_size, shuffle=True, pin_memory=True)\n",
        "  validation_loader = DataLoader(validation_set, batch_size=64, pin_memory=True)\n",
        "  ## Using a raw DataLoader, rather than LightningDataModule, for greater transparency\n",
        "\n",
        "  # Set up model\n",
        "  model = MNIST_LitModule(n_layer_1=128, n_layer_2=128, optim = wandb.config.optimizer, lr = wandb.config.learning_rate)\n",
        "  wandb.watch(model)\n",
        "\n",
        "\n",
        "  trainer = Trainer(gpus=1, max_epochs=5,logger=wandb_logger,\n",
        "                            callbacks=[\n",
        "                                      log_predictions_callback\n",
        "                                      ], \n",
        "                        precision=32)\n",
        "  trainer.fit(model, training_loader, validation_loader)\n",
        "  wandb.finish()"
      ],
      "metadata": {
        "id": "_8gQQNExLUWS"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_id = wandb.sweep(sweep_config, project=\"ptl-sweeps-example-v2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0LtiJ_W-LUSW",
        "outputId": "0829c472-b98a-405e-cd5c-6274ed48c18a"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: w35g2jx9\n",
            "Sweep URL: https://wandb.ai/tim-w/ptl-sweeps-example-v2/sweeps/w35g2jx9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wandb_agent = wandb.agent(sweep_id, function=sweep_train, count = 3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_izDwqDLUIc",
        "outputId": "b40a316e-8e19-426c-9a59-543509e36eb7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: fbdo5vuh with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.05\n",
            "Run fbdo5vuh errored: Error('You must call wandb.init() before wandb.config.batch_size')\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run fbdo5vuh errored: Error('You must call wandb.init() before wandb.config.batch_size')\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: xpvun7we with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.025\n",
            "Run xpvun7we errored: Error('You must call wandb.init() before wandb.config.batch_size')\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run xpvun7we errored: Error('You must call wandb.init() before wandb.config.batch_size')\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: htcio4b4 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
            "Run htcio4b4 errored: Error('You must call wandb.init() before wandb.config.batch_size')\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run htcio4b4 errored: Error('You must call wandb.init() before wandb.config.batch_size')\n",
            "Detected 3 failed runs in the first 60 seconds, killing sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Detected 3 failed runs in the first 60 seconds, killing sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: To disable this check set WANDB_AGENT_DISABLE_FLAPPING=true\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lnQKK5loh12h"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Sweeps with Pytorch Lightning models.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}